{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvDEVuJtQanJEH4AoJwj8Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bagmitadas/ML/blob/main/ML_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Simple Linear Regression?\n",
        "\n",
        "Answer: Simple Linear Regression is a statistical method used to model the relationship between two variables. It assumes a linear relationship, where one variable (the dependent variable) is predicted from another (the independent variable).\n",
        "\n",
        "Q2. What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "Answer: Key assumptions of Simple Linear Regression include:\n",
        "\n",
        "Linearity: The relationship between the independent and dependent variables is linear.\n",
        "Independence: The errors (residuals) are independent of each other.  \n",
        "Homoscedasticity: The errors have constant variance across all levels of the independent variable.\n",
        "Normality: The errors are normally distributed.\n",
        "No or little multicollinearity: There is no or little correlation between the independent variables. (This is more relevant in multiple regression, but applies in a simple form here as well.)\n",
        "\n",
        "Q3. What does the coefficient m represent in the equation Y=mX+c?\n",
        "\n",
        "Answer: The coefficient \"m\" represents the slope in the equation Y=mX+c. The slope indicates the change in the dependent variable (Y) for a one-unit change in the independent variable (X).\n",
        "\n",
        "Q4. What does the intercept c represent in the equation  Y=mX+c?\n",
        "\n",
        "Answer: The intercept \"c\" represents the y-intercept in the equation Y=mX+c. It is the value of the dependent variable (Y) when the independent variable (X) is zero.\n",
        "\n",
        "Q5. How do we calculate the slope m in Simple Linear Regression?\n",
        "\n",
        "Answer: The slope (m) in Simple Linear Regression is calculated using the formula:\n",
        "\n",
        "m = Σ[(Xi - X̄)(Yi - Ȳ)] / Σ[(Xi - X̄)²]\n",
        "\n",
        "Where:\n",
        "\n",
        "* Xi are the values of the independent variable.\n",
        "* X̄ is the mean of the independent variable.\n",
        "* Yi are the values of the dependent variable.\n",
        "* Ȳ is the mean of the dependent variable.\n",
        "\n",
        "Q6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "\n",
        "Answer: The purpose of the least squares method in Simple Linear Regression is to find the line of best fit that minimizes the sum of the squared differences between the observed values and the values predicted by the regression line.\n",
        "\n",
        "Q7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "\n",
        "Answer: The coefficient of determination (R²) is interpreted as the proportion of the variance in the dependent variable (Y) that is explained by the independent variable (X). It ranges from 0 to 1, where a higher value indicates a better fit of the model to the data.\n",
        "\n",
        "Q8. What is Multiple Linear Regression?\n",
        "\n",
        "Answer: Multiple Linear Regression is a statistical technique that extends Simple Linear Regression to model the relationship between one dependent variable and two or more independent variables.\n",
        "\n",
        "Q9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "\n",
        "Answer: The main difference is the number of independent variables used to predict the dependent variable. Simple Linear Regression uses one independent variable, while Multiple Linear Regression uses two or more.  \n",
        "\n",
        "Q10. What are the key assumptions of Multiple Linear Regression?\n",
        "\n",
        "Answer: The key assumptions of Multiple Linear Regression are similar to those of Simple Linear Regression:\n",
        "\n",
        "Linearity: The relationship between the dependent variable and each independent variable is linear (when other independent variables are held constant).\n",
        "Independence: The errors (residuals) are independent of each other.\n",
        "Homoscedasticity: The errors have constant variance across all levels of the independent variables.\n",
        "Normality: The errors are normally distributed.\n",
        "No or little multicollinearity: There is no or little high correlation between the independent variables.\n",
        "\n",
        "Q11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "\n",
        "Answer: Heteroscedasticity is a violation of the assumption of homoscedasticity. It occurs when the variance of the errors is not constant across all levels of the independent variables. Heteroscedasticity can lead to inefficient and biased estimates of the regression coefficients, as well as unreliable hypothesis tests and confidence intervals.  \n",
        "\n",
        "Q12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "\n",
        "Answer: Several techniques can be used to address high multicollinearity:\n",
        "\n",
        "Remove one or more of the highly correlated independent variables.\n",
        "Combine the correlated variables into a single variable.\n",
        "Use dimensionality reduction techniques like Principal Component Analysis (PCA).\n",
        "Use penalized regression techniques like Ridge Regression or Lasso Regression.\n",
        "Gather more data, which can sometimes help to reduce the correlation effects.\n",
        "\n",
        "Q13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "\n",
        "Answer: Common techniques for transforming categorical variables include:\n",
        "\n",
        "Dummy Coding (One-Hot Encoding): Creating binary (0 or 1) indicator variables for each category.\n",
        "Effect Coding (Deviation Coding): Assigning codes like -1, 0, and 1 to represent different categories relative to the overall mean.\n",
        "Contrast Coding: Using specific sets of weights to test particular hypotheses about the differences between category means.\n",
        "\n",
        "Q14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "\n",
        "Answer: Interaction terms in Multiple Linear Regression allow the relationship between an independent variable and the dependent variable to vary depending on the level of another independent variable. They capture synergistic or antagonistic effects between predictors.\n",
        "\n",
        "Q15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "\n",
        "Answer: In Simple Linear Regression, the intercept is the value of the dependent variable when the single independent variable is zero. In Multiple Linear Regression, the intercept is the value of the dependent variable when all independent variables are zero. The interpretation in multiple regression is contingent on whether it's meaningful for all predictors to simultaneously be zero.  \n",
        "\n",
        "Q16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "\n",
        "Answer: The slope in regression analysis indicates the magnitude and direction of the relationship between an independent variable and the dependent variable. A positive slope means that as the independent variable increases, the dependent variable tends to increase, and vice versa for a negative slope. The slope is crucial for making predictions, as it determines how much the predicted value of the dependent variable changes for a given change in the independent variable.\n",
        "\n",
        "Q17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "\n",
        "Answer: The intercept provides a baseline value for the dependent variable when all independent variables are zero. While sometimes this value has a direct practical interpretation, in other cases, it serves more as a necessary component of the model to position the regression line correctly within the data space. It helps define the starting point of the predicted relationship.\n",
        "\n",
        "Q18. What are the limitations of using R² as a sole measure of model performance?\n",
        "\n",
        "Answer: Limitations of using R² alone include:\n",
        "\n",
        "It doesn't indicate if the model is correctly specified. A high R² doesn't guarantee a good model fit if the underlying relationship is not linear or if important variables are omitted.\n",
        "It doesn't prevent overfitting. Adding more predictors will always increase R², even if those predictors don't truly improve the model.\n",
        "It doesn't tell you about the significance of individual predictors. A high R² can occur even if some independent variables are not statistically significant.\n",
        "It is sensitive to outliers. Outliers can inflate or deflate the R² value.\n",
        "\n",
        "Q19. How would you interpret a large standard error for a regression coefficient?\n",
        "\n",
        "Answer: A large standard error for a regression coefficient indicates that there is a wide range of plausible values for the true coefficient in the population. It suggests that the estimated coefficient is not very precise and that the independent variable might not be a statistically significant predictor of the dependent variable in the model.\n",
        "\n",
        "Q20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "\n",
        "Answer: Heteroscedasticity can often be identified in residual plots as a non-constant variance of the residuals as the predicted values of the dependent variable increase. Common patterns include a funnel shape (where the spread of residuals increases or decreases). It's important to address heteroscedasticity because it violates a key assumption of linear regression, leading to unreliable standard errors, hypothesis tests, and confidence intervals for the regression coefficients.\n",
        "\n",
        "Q21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "\n",
        "Answer: A high R² and a low adjusted R² suggest that the model may be overfitting the data. The high R² indicates that the model explains a large proportion of the variance in the dependent variable in the sample data. However, the low adjusted R² (which penalizes the inclusion of unnecessary predictors) suggests that many of the added independent variables do not significantly contribute to explaining the variance and may not generalize well to new data.\n",
        "\n",
        "Q22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "\n",
        "Answer: Scaling variables in Multiple Linear Regression can be important for several reasons:\n",
        "\n",
        "To prevent variables with larger scales from dominating the model.\n",
        "To improve the performance of some optimization algorithms used in fitting the model.\n",
        "To make the coefficients more easily interpretable, especially when using standardized coefficients.\n",
        "To help with numerical stability in computations.\n",
        "It can be particularly useful when using penalized regression techniques like Ridge and Lasso.\n",
        "\n",
        "Q23. What is polynomial regression?\n",
        "\n",
        "Answer: Polynomial regression is a form of regression analysis in which the relationship between the independent variable (X) and the dependent variable (Y) is modeled as an nth-degree polynomial. It is used to model non-linear relationships between variables.\n",
        "\n",
        "Q24. How does polynomial regression differ from linear regression?\n",
        "\n",
        "Answer: Linear regression models a linear relationship (a straight line), while polynomial regression models a curvilinear relationship using polynomial terms (e.g., X², X³).\n",
        "\n",
        "Q25. When is polynomial regression used?\n",
        "\n",
        "Answer: Polynomial regression is used when the relationship between the independent and dependent variables appears to be non-linear, as suggested by scatter plots. It can capture curves and bends in the data that a straight line cannot.\n",
        "\n",
        "Q26. What is the general equation for polynomial regression?\n",
        "\n",
        "Answer: The general equation for polynomial regression of degree n is:\n",
        "\n",
        "Y=β0+β1X+β2X2+β3X3+...+βnXn+ϵ\n",
        "\n",
        "Where:\n",
        "\n",
        "* Y is the dependent variable.\n",
        "* X is the independent variable.\n",
        "* <span class=\"math-inline\">\\\\beta\\_0, \\\\beta\\_1, \\.\\.\\., \\\\beta\\_n</span> are the regression coefficients.\n",
        "* <span class=\"math-inline\">\\\\epsilon</span> is the error term.\n",
        "\n",
        "Q27. Can polynomial regression be applied to multiple variables?\n",
        "\n",
        "Answer: Yes, polynomial regression can be extended to multiple independent variables. This involves including polynomial terms and interaction terms of the multiple predictors. However, the complexity of the model increases rapidly with higher degrees and more variables.\n",
        "\n",
        "Q28. What are the limitations of polynomial regression?\n",
        "\n",
        "Answer: Limitations of polynomial regression include:\n",
        "\n",
        "Overfitting: High-degree polynomials can easily overfit the data, capturing noise rather than the underlying relationship and leading to poor generalization.\n",
        "Extrapolation: Polynomial models can produce wildly inaccurate predictions outside the range of the observed data.\n",
        "Interpretation: The coefficients of higher-degree terms can be difficult to interpret in a practical sense.\n",
        "Choosing the degree of the polynomial: Selecting the appropriate degree can be challenging.\n",
        "\n",
        "Q29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "\n",
        "Answer: Methods to evaluate model fit when selecting the degree of a polynomial include:\n",
        "\n",
        "Visual inspection of the fitted curve on a scatter plot.\n",
        "Using statistical measures like Adjusted R², AIC (Akaike Information Criterion), and BIC (Bayesian Information Criterion), which penalize model complexity.\n",
        "Cross-validation techniques to assess the model's performance on unseen data.\n",
        "Partial F-tests to compare models of different degrees.\n",
        "\n",
        "Q30. Why is visualization important in polynomial regression?\n",
        "\n",
        "Answer: Visualization is crucial in polynomial regression to:\n",
        "\n",
        "Identify non-linear relationships in the data.\n",
        "Assess the fit of the polynomial curve to the data.\n",
        "Detect potential overfitting or underfitting.\n",
        "Help in choosing an appropriate degree for the polynomial.\n",
        "Identify outliers or unusual patterns in the residuals.\n",
        "\n",
        "Q31. How is polynomial regression implemented in Python?\n",
        "\n",
        "Answer: Polynomial regression can be implemented in Python using libraries like scikit-learn. Typically, this involves:\n",
        "\n",
        "Generating polynomial features from the original independent variable(s) using PolynomialFeatures from sklearn.preprocessing.\n",
        "Fitting a linear regression model (e.g., LinearRegression from sklearn.linear_model) to these polynomial features."
      ],
      "metadata": {
        "id": "LXv1kXiycdX_"
      }
    }
  ]
}