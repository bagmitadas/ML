{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1xdNHXoR1y7I3qRKLmkyQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bagmitadas/ML/blob/main/LogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*** Theoretical Questions ***\n",
        "\n",
        "Q1. What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "\n",
        "Answer. Logistic Regression is a statistical method used for binary classification problems, predicting the probability of a binary outcome (0 or 1).\n",
        "While Linear Regression predicts a continuous output, Logistic Regression predicts the probability of a categorical outcome.  It uses a sigmoid function to map the linear combination of inputs to a probability between 0 and 1.\n",
        "\n",
        "Q2. What is the mathematical equation of Logistic Regression?\n",
        "\n",
        "Answer. The mathematical equation of Logistic Regression is typically represented as: p = 1 / (1 + e&lt;sup>-z&lt;/sup>), where 'p' is the probability of the positive class, 'e' is the base of the natural logarithm, and 'z' is the linear combination of the input features and their coefficients (z = b&lt;sub>0&lt;/sub> + b&lt;sub>1&lt;/sub>x&lt;sub>1&lt;/sub> + b&lt;sub>2&lt;/sub>x&lt;sub>2&lt;/sub> + ... + b&lt;sub>n&lt;/sub>x&lt;sub>n&lt;/sub>).  \n",
        "\n",
        "Q3.Why do we use the Sigmoid function in Logistic Regression?\n",
        "\n",
        "Answer. The Sigmoid function is used in Logistic Regression to map the predicted linear combination of input features into a probability value between 0 and 1.  This makes it suitable for binary classification, where we need to predict the likelihood of an instance belonging to a particular class.\n",
        "\n",
        "Q4.What is the cost function of Logistic Regression?\n",
        "\n",
        "Answer. The cost function of Logistic Regression is typically the Cross-Entropy Loss (also known as Log Loss).  It measures the error between predicted probabilities and the actual binary outcomes.  \n",
        "\n",
        "Q5.What is Regularization in Logistic Regression? Why is it needed?\n",
        "\n",
        "Answer. Regularization in Logistic Regression is a technique used to prevent overfitting.\n",
        "It adds a penalty term to the cost function to discourage large coefficients, making the model simpler and less prone to fitting noise in the training data.  \n",
        "\n",
        "Q6.Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
        "\n",
        "Answer. Lasso (L1 regularization) adds the absolute value of the coefficients to the penalty term, which can shrink some coefficients to exactly zero, effectively performing feature selection.  \n",
        "Ridge (L2 regularization) adds the squared value of the coefficients to the penalty term, which shrinks coefficients towards zero but rarely exactly to zero.   \n",
        "Elastic Net is a combination of both L1 and L2 regularization, adding both the absolute and squared values of the coefficients to the penalty term.  It balances the feature selection of Lasso and the coefficient shrinkage of Ridge.\n",
        "\n",
        "Q7.When should we use Elastic Net instead of Lasso or Ridge?\n",
        "\n",
        "Answer. Elastic Net is preferred when there are many correlated features.  It can group correlated features together, selecting some and shrinking others, unlike Lasso which tends to select only one.  It also provides a balance between Lasso and Ridge, potentially leading to better performance than either alone.  \n",
        "\n",
        "Q8.What is the impact of the regularization parameter (λ) in Logistic Regression?\n",
        "\n",
        "Answer. The regularization parameter (λ, often represented as 'C' in scikit-learn, where C = 1/λ) controls the strength of regularization.\n",
        "A larger λ (smaller C) increases regularization, leading to simpler models with smaller coefficients, which can help prevent overfitting.  A smaller λ (larger C) reduces regularization, allowing the model to fit the training data more closely.\n",
        "\n",
        "Q9.What are the key assumptions of Logistic Regression?\n",
        "\n",
        "Answer. Key assumptions of Logistic Regression include:\n",
        "Binary or ordinal dependent variable.  \n",
        "Independence of observations.   \n",
        "Linearity between the independent variables and the log-odds of the outcome.  \n",
        "Sufficiently large sample size.   \n",
        "No multicollinearity among independent variables.  \n",
        "\n",
        "Q10. What are some alternatives to Logistic Regression for classification tasks?\n",
        "\n",
        "Answer. Alternatives to Logistic Regression for classification tasks include:\n",
        "Decision Trees.\n",
        "Support Vector Machines (SVMs).\n",
        "Naive Bayes.\n",
        "K-Nearest Neighbors (KNN).\n",
        "Random Forest.\n",
        "Gradient Boosting algorithms.\n",
        "Neural Networks.  \n",
        "\n",
        "Q11.What are Classification Evaluation Metrics?\n",
        "\n",
        "Answer. Classification evaluation metrics are used to assess the performance of a classification model.   \n",
        "Common metrics include accuracy, precision, recall, F1-score, and ROC-AUC.\n",
        "\n",
        "Q12.How does class imbalance affect Logistic Regression?\n",
        "\n",
        "Answer. Class imbalance, where one class has significantly more samples than the other, can negatively impact Logistic Regression.  \n",
        "It can lead to biased models that perform well on the majority class but poorly on the minority class.  \n",
        "\n",
        "Q13. What is Hyperparameter Tuning in Logistic Regression?\n",
        "\n",
        "Answer. Hyperparameter tuning in Logistic Regression involves selecting the best values for parameters that are not learned from the data, such as the regularization strength (C) and the penalty type.  \n",
        "Techniques like Grid Search and Randomized Search are used to find the optimal combination of hyperparameters.  \n",
        "\n",
        "Q14.What are different solvers in Logistic Regression? Which one should be used?\n",
        "\n",
        "Answer. Different solvers in Logistic Regression are algorithms used to optimize the cost function.  \n",
        "Common solvers include 'liblinear', 'lbfgs', 'sag', 'saga', and 'newton-cg'.\n",
        "The choice of solver depends on the dataset size, the penalty type, and whether you need multiclass classification.  For example, 'liblinear' is suitable for small datasets, while 'saga' is good for large datasets.  \n",
        "\n",
        "Q15.How is Logistic Regression extended for multiclass classification?\n",
        "\n",
        "Answer. Logistic Regression can be extended for multiclass classification using techniques like:\n",
        "One-vs-Rest (OvR): Training a separate binary classifier for each class against all other classes.   \n",
        "One-vs-One (OvO): Training a binary classifier for every pair of classes.\n",
        "Softmax Regression: A generalization of Logistic Regression that directly handles multiple classes.\n",
        "\n",
        "Q16. What are the advantages and disadvantages of Logistic Regression?\n",
        "\n",
        "Advantages:\n",
        "Simple to implement and interpret.\n",
        "Efficient to train.\n",
        "Provides probability estimates.\n",
        "Disadvantages:\n",
        "Assumes linearity between features and log-odds.  \n",
        "Can struggle with complex non-linear relationships.   \n",
        "Sensitive to multicollinearity.  \n",
        "\n",
        "Q17. What are some use cases of Logistic Regression?\n",
        "\n",
        "Answer. Use cases of Logistic Regression include:\n",
        "Medical diagnosis (predicting disease presence).  \n",
        "Spam detection.\n",
        "Customer churn prediction.\n",
        "Credit risk assessment.\n",
        "Sentiment analysis.\n",
        "\n",
        "Q18.What is the difference between Softmax Regression and Logistic Regression?\n",
        "\n",
        "Answer. Logistic Regression is used for binary classification, while Softmax Regression is a generalization of Logistic Regression for multiclass classification.\n",
        "Softmax Regression assigns probabilities to multiple classes, and the sum of probabilities across all classes is 1.\n",
        "\n",
        "Q19.How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "\n",
        "Answer. The choice between OvR and Softmax depends on the specific problem.\n",
        "Softmax is preferred when the classes are mutually exclusive.  * OvR can be suitable when the classes are not mutually exclusive.  \n",
        "\n",
        "Q20. How do we interpret coefficients in Logistic Regression?\n",
        "\n",
        "Answer. Coefficients in Logistic Regression represent the change in the log-odds of the outcome for a one-unit change in the predictor variable, holding other variables constant.  \n",
        "A positive coefficient indicates that an increase in the predictor increases the log-odds of the outcome, while a negative coefficient indicates a decrease."
      ],
      "metadata": {
        "id": "kpblfNQ2knKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PRACTICAL\n",
        "''' Q1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy.'''\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris  # Using iris dataset for example\n",
        "\n",
        "# Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply Logistic Regression\n",
        "model = LogisticRegression(solver='liblinear', multi_class='ovr')  # You can change the solver\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "LBtPpWgjkqSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy.'''\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply Logistic Regression with L1 regularization\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', multi_class='ovr', C=1.0) # You can adjust C\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy with L1 Regularization:\", accuracy)"
      ],
      "metadata": {
        "id": "wp9qz0nmlO2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q3.Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients.'''\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression with L2 regularization\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', multi_class='ovr', C=1.0)  # You can adjust C and solver\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy with L2 Regularization:\", accuracy)\n",
        "\n",
        "# Print coefficients\n",
        "print(\"Model Coefficients:\", model.coef_)"
      ],
      "metadata": {
        "id": "i9NfWHmXlUu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet').\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression with Elastic Net regularization\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', multi_class='ovr', C=1.0, l1_ratio=0.5) # You MUST use 'saga' or 'elasticnet' with 'saga'.  Adjust C and l1_ratio\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy with Elastic Net Regularization:\", accuracy)"
      ],
      "metadata": {
        "id": "pNJJ2LZWldq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q5. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'.\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression with OvR multiclass\n",
        "model = LogisticRegression(solver='liblinear', multi_class='ovr', C=1.0)  # You can change the solver and C\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy with OvR Multiclass:\", accuracy)"
      ],
      "metadata": {
        "id": "eH1CzoQAlmXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy.\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'penalty': ['l1', 'l2', 'elasticnet', None], 'solver': ['liblinear', 'saga', 'lbfgs']}  # Added 'None' to penalty\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid_search = GridSearchCV(LogisticRegression(multi_class='ovr'), param_grid, cv=3, verbose=0) # Added verbose=0\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Predict with best model\n",
        "y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Best Model Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "mGTHmjiFlvYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy.\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Apply Stratified K-Fold Cross-Validation\n",
        "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42) # You can change the number of splits\n",
        "accuracies = []\n",
        "\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "        model = LogisticRegression(solver='liblinear', multi_class='ovr')  # You can change the solver\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        accuracies.append(accuracy)\n",
        "\n",
        "# Print the average accuracy\n",
        "average_accuracy = np.mean(accuracies)\n",
        "print(\"Average Accuracy from Stratified K-Fold CV:\", average_accuracy)\n",
        ""
      ],
      "metadata": {
        "id": "EM_M6FHcl5G_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset from a CSV file\n",
        "data = pd.read_csv('PeopleData.csv')\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = data.drop('Last Name', axis=1)\n",
        "y = data['Last Name']\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply Logistic Regression\n",
        "model = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy on CSV Data:\", accuracy)"
      ],
      "metadata": {
        "id": "cUpo09bqmmnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy.**\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import uniform, loguniform  # For parameter distributions\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define hyperparameter distributions for RandomizedSearchCV\n",
        "param_distributions = {\n",
        "'C': loguniform(0.001, 100),  #  Log-uniform distribution for C\n",
        "        'penalty': ['l1', 'l2', 'elasticnet', None],\n",
        "        'solver': ['liblinear', 'saga', 'lbfgs']\n",
        "    }\n",
        "\n",
        "# Apply RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(LogisticRegression(multi_class='ovr'),\n",
        "                                     param_distributions,\n",
        "                                     n_iter=10,  # Number of random samples\n",
        "                                     cv=3,\n",
        "                                     random_state=42,\n",
        "                                     n_jobs=-1)  # Use all available cores\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "\n",
        "# Predict with best model\n",
        "y_pred = random_search.best_estimator_.predict(X_test)\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Best Model Accuracy (RandomizedSearchCV):\", accuracy)\n",
        ""
      ],
      "metadata": {
        "id": "GEhWXWvLl_6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "\n",
        "#Load data\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Implement OvO Multiclass Logistic Regression\n",
        "ovo_model = OneVsOneClassifier(LogisticRegression(solver='liblinear', C=1.0))  #  You can adjust solver and C\n",
        "ovo_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = ovo_model.predict(X_test)\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"OvO Multiclass Logistic Regression Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "YeL1mVUMl_4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer  # Using breast cancer dataset for binary classification\n",
        "import seaborn as sns\n",
        "\n",
        "# Load data (binary classification dataset)\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(solver='liblinear')  # Good for binary classification\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "0SNPABtG7O-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Q12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load data (binary classification dataset)\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Precision, Recall, and F1-Score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print the metrics\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1)"
      ],
      "metadata": {
        "id": "jykrVxOi7inu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.datasets import make_classification  # For generating an imbalanced dataset\n",
        "\n",
        "# Generate an imbalanced dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
        "                           n_redundant=5, weights=[0.9, 0.1],  # 90% class 0, 10% class 1\n",
        "                           random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)  # Stratify to keep imbalance\n",
        "\n",
        "# Train Logistic Regression WITHOUT class weights (baseline)\n",
        "model1 = LogisticRegression(solver='liblinear')\n",
        "model1.fit(X_train, y_train)\n",
        "y_pred1 = model1.predict(X_test)\n",
        "print(\"--- Without Class Weights ---\")\n",
        "print(classification_report(y_test, y_pred1))\n",
        "\n",
        "\n",
        "# Train Logistic Regression WITH class weights\n",
        "model2 = LogisticRegression(solver='liblinear', class_weight='balanced')  # Apply class weights\n",
        "model2.fit(X_train, y_train)\n",
        "y_pred2 = model2.predict(X_test)\n",
        "print(\"\\n--- With Class Weights ---\")\n",
        "print(classification_report(y_test, y_pred2))\n",
        "\n",
        "\n",
        "# Q14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.impute import SimpleImputer  # For handling missing values\n",
        "\n",
        "# Load the Titanic dataset\n",
        "titanic_data = pd.read_csv('titanic.csv')  # Replace 'titanic.csv' with the actual filename\n",
        "\n",
        "# Select features and target (you might need to adjust these based on your dataset)\n",
        "X = titanic_data[['Pclass', 'Age', 'Sex', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
        "y = titanic_data['Survived']\n",
        "\n",
        "# Handle categorical features\n",
        "X['Sex'] = X['Sex'].map({'male': 0, 'female': 1})\n",
        "X['Embarked'] = X['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})  # Or use one-hot encoding\n",
        "\n",
        "# Handle missing values using SimpleImputer\n",
        "imputer = SimpleImputer(strategy='median')  # Or 'mean', 'most_frequent'\n",
        "X['Age'] = imputer.fit_transform(X[['Age']])\n",
        "X['Embarked'] = imputer.fit_transform(X[['Embarked']])\n",
        "X = X.fillna(X.median()) #  Another way to fill missing values\n",
        "\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "print(\"Titanic Dataset - Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Titanic Dataset - Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "TmxJrpGF7s7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Q15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model.\n",
        "# Evaluate its accuracy and compare results with and without scaling.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression WITHOUT scaling\n",
        "model_no_scale = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
        "model_no_scale.fit(X_train, y_train)\n",
        "y_pred_no_scale = model_no_scale.predict(X_test)\n",
        "accuracy_no_scale = accuracy_score(y_test, y_pred_no_scale)\n",
        "print(\"Accuracy WITHOUT Scaling:\", accuracy_no_scale)\n",
        "\n",
        "# Apply StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression WITH scaling\n",
        "model_scaled = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(\"Accuracy WITH Scaling:\", accuracy_scaled)"
      ],
      "metadata": {
        "id": "LJgommaS7zwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load data (binary classification dataset)\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
        "\n",
        "# Calculate ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(\"ROC-AUC Score:\", roc_auc)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "maEaRNlW8GLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression with custom C (inverse of regularization strength)\n",
        "custom_C = 0.5\n",
        "model = LogisticRegression(solver='liblinear', multi_class='ovr', C=custom_C)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy with C={custom_C}:\", accuracy)\n",
        "\n"
      ],
      "metadata": {
        "id": "wg97uF_Q8KZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q18. Write a Python program to train Logistic Regression and identify important features based on model coefficients.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression (using one-vs-rest for multiclass)\n",
        "model = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get coefficients (for OvR, there will be coefficients for each class)\n",
        "coefficients = model.coef_\n",
        "\n",
        "# Identify important features (magnitude of coefficients indicates importance)\n",
        "print(\"Feature Importance (based on absolute coefficient magnitude for each class):\")\n",
        "for i, class_coefs in enumerate(coefficients):\n",
        "    print(f\"Class {i}:\")\n",
        "    feature_importance = sorted(zip(feature_names, abs(class_coefs)), key=lambda x: x[1], reverse=True)\n",
        "    for feature, importance in feature_importance:\n",
        "        print(f\"  {feature}: {importance:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "BtWWvElJ8REs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen's Kappa Score.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Cohen's Kappa Score\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"Cohen's Kappa Score:\", kappa_score)\n"
      ],
      "metadata": {
        "id": "UuSl0_0Q8Z9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load data (binary classification dataset)\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate Precision-Recall curve\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
        "\n",
        "# Calculate average precision score\n",
        "average_precision = average_precision_score(y_test, y_pred_proba)\n",
        "print(f\"Average Precision-Recall Score: {average_precision:.2f}\")\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, color='blue', lw=2, label=f'Precision-Recall curve (AP = {average_precision:.2f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jzDJ7cIR8gPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define solvers to compare\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "accuracies = {}\n",
        "\n",
        "# Train and evaluate Logistic Regression for each solver\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(solver=solver, multi_class='ovr', max_iter=10000)  # Increase max_iter to ensure convergence\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies[solver] = accuracy\n",
        "    print(f\"Accuracy with solver '{solver}': {accuracy}\")\n",
        "\n",
        "# Compare accuracies\n",
        "best_solver = max(accuracies, key=accuracies.get)\n",
        "print(f\"\\nBest Solver: {best_solver} with Accuracy: {accuracies[best_solver]}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "xzQ80VkG8vSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC).\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load data (binary classification dataset)\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate MCC\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(\"Matthews Correlation Coefficient (MCC):\", mcc)\n"
      ],
      "metadata": {
        "id": "vz8howE28yzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q23. Write a Python program to train Logistic Regression on both raw and standardized data.\n",
        "# Compare their accuracy to see the impact of feature scaling.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression on RAW data\n",
        "model_raw = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "print(\"Accuracy on Raw Data:\", accuracy_raw)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression on STANDARDIZED data\n",
        "model_scaled = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(\"Accuracy on Standardized Data:\", accuracy_scaled)\n",
        "\n",
        "# Compare accuracies\n",
        "print(\"\\nComparison:\")\n",
        "print(f\"Difference in Accuracy: {accuracy_scaled - accuracy_raw}\")\n"
      ],
      "metadata": {
        "id": "RRVqIxSG85YC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a range of C values to test\n",
        "C_values = [0.001, 0.01, 0.1, 1, 10, 100]\n",
        "best_C = None\n",
        "best_accuracy = 0\n",
        "\n",
        "# Use K-Fold Cross-Validation to find the optimal C\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)  # You can adjust the number of folds\n",
        "\n",
        "for C in C_values:\n",
        "    model = LogisticRegression(solver='liblinear', multi_class='ovr', C=C, max_iter=10000)\n",
        "    scores = cross_val_score(model, X_train, y_train, cv=kf, scoring='accuracy')  # Use cross_val_score\n",
        "    avg_accuracy = np.mean(scores)\n",
        "\n",
        "    print(f\"C={C}, Average Accuracy (CV): {avg_accuracy}\")\n",
        "\n",
        "    if avg_accuracy > best_accuracy:\n",
        "        best_accuracy = avg_accuracy\n",
        "        best_C = C\n",
        "\n",
        "print(f\"\\nOptimal C: {best_C} with Best Average Accuracy: {best_accuracy}\")\n"
      ],
      "metadata": {
        "id": "a-2rgE3R9Bvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib  # For saving and loading the model\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Save the trained model\n",
        "filename = 'logistic_regression_model.joblib'\n",
        "joblib.dump(model, filename)\n",
        "print(f\"Model saved as {filename}\")\n",
        "\n",
        "# Load the model\n",
        "loaded_model = joblib.load(filename)\n",
        "print(f\"Model loaded from {filename}\")\n",
        "\n",
        "# Make predictions using the loaded model\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy of Loaded Model:\", accuracy)"
      ],
      "metadata": {
        "id": "Ye0dwWNz9N0R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}